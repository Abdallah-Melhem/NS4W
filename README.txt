ğŸ§¬ Genetic Disorder Prediction Chatbot (Local LLM + ML)

An interactive web-based chatbot that collects medical information from users through natural conversation, validates missing data, runs a machine-learning prediction model, and explains the result in simple, human-readable language.

The system runs fully locally using:

Node.js + Express (chat logic & session handling)

Ollama (LLaMA 3.2) as a local LLM

Flask + CatBoost for machine-learning prediction

HTML/CSS/JS frontend chat interface

âœ¨ Key Features

ğŸ’¬ Natural conversation with users (free text, any order)

ğŸ§  Local LLM (no external APIs, no costs, no data leakage)

ğŸ§¾ Automatic extraction of medical values from text

â“ Smart follow-up questions when data is missing

ğŸ“Š ML prediction using a trained CatBoost model

ğŸ§‘â€âš•ï¸ Human-friendly explanation of results

ğŸ” Session-based chat with status and reset commands

ğŸ—ï¸ System Architecture
Browser (Chat UI)
      â”‚
      â–¼
Node.js / Express
(API + Session + LLM logic)
      â”‚
      â”œâ”€â”€ Ollama (llama3.2)
      â”‚   â””â”€ Extracts data & humanizes output
      â”‚
      â””â”€â”€ Flask API (/predict)
          â””â”€ CatBoost ML model

ğŸ§  Required Medical Inputs

Core (required):

Blood cell count

White blood cell count

Patient age

Motherâ€™s age

Fatherâ€™s age

Optional:

Genes in motherâ€™s side (yes/no)

Inherited from father (yes/no)

Birth asphyxia (yes/no)

Substance abuse (yes/no)

Paternal gene (yes/no)

Number of previous abortions

The chatbot will automatically ask for anything missing.

ğŸ› ï¸ Tech Stack
Backend

Node.js (ES modules)

Express

express-session

node-fetch

Ollama (local LLM)

ML API

Python

Flask

CatBoost

Pandas / NumPy

Frontend

HTML

CSS

Vanilla JavaScript (Fetch API)

ğŸš€ Setup Instructions
1ï¸âƒ£ Install Ollama & Pull Model
ollama run llama3.2


Verify Ollama is running:

http://127.0.0.1:11434

2ï¸âƒ£ Flask (ML API)
Install dependencies
pip install flask flask-cors pandas numpy catboost

Run Flask server
python app.py


Flask will run at:

http://127.0.0.1:5000


Health check:

GET /health

3ï¸âƒ£ Node.js Backend
Install dependencies
npm install

Create .env
PORT=3000
FLASK_API_URL=http://127.0.0.1:5000
SESSION_SECRET=your_secret_here

OLLAMA_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3.2:latest

Start server
node server.js


Node server runs at:

http://localhost:3000

4ï¸âƒ£ Open the App

Open your browser and go to:

http://localhost:3000

ğŸ’¬ Example Conversation

User:

My age is 20, mom is 45

Bot:

I still need blood cell count, white blood cell count, and fatherâ€™s age.

User:

blood cells 4.7, white blood cells 7.9, dad 50

Bot:

(Simple explanation of prediction)
Result:
â€¢ Prediction: X
â€¢ Confidence: Y%

ğŸ§ª Useful Commands

status â†’ shows currently collected data

reset â†’ clears the session and starts over

ğŸ”’ Privacy & Safety

All processing is local

No data is sent to external servers

No medical advice is given

Results are informational only

ğŸ“Œ Notes

Make sure the CatBoost model was trained with the same feature names and categories.

Ollama must be running before starting Node.js.

If Ollama is slow, the frontend includes request timeouts and error handling.

ğŸ“„ License

This project is for educational and research purposes.
Not intended for real medical diagnosis or treatment.